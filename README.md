# Vibe Coded Local Python Agent Using Ollama

A Python agent that generates original TV episode scripts (Markdown format) inspired by *The Expanse*, powered by a locally running Ollama LLM.

> **Note:** This codebase was entirely generated by GitHub Copilot.

## Table of Contents

- [Requirements](#requirements)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Troubleshooting](#troubleshooting)

---

## Requirements

- **Python 3.8+** (tested on Python 3.10+)
- **pip** (Python package manager)
- **Ollama** running locally on your machine (default: `http://127.0.0.1:11434`)

### Installing Ollama

1. Download Ollama from [ollama.ai](https://ollama.ai)
2. Install and start the Ollama service
3. Pull a model (recommended: `gemma3:1b` for faster generation):
   ```bash
   ollama pull gemma3:1b
   ```
4. Verify Ollama is running:
   ```bash
   curl http://127.0.0.1:11434/
   ```
   You should see: `Ollama is running`

---

## Installation

### 1. Clone or Navigate to the Repository

```bash
cd /path/to/agentic_python
```

### 2. Create a Virtual Environment

```bash
python3 -m venv venv
```

### 3. Activate the Virtual Environment

**macOS / Linux:**
```bash
source venv/bin/activate
```

**Windows:**
```bash
venv\Scripts\activate
```

### 4. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## Configuration

### Ollama Server Address

By default, the agent connects to `http://127.0.0.1:11434`.

To use a different address/port, set the `OLLAMA_URL` environment variable:

**macOS / Linux:**
```bash
export OLLAMA_URL=http://your-server:11434
```

**Windows (CMD):**
```cmd
set OLLAMA_URL=http://your-server:11434
```

Alternatively, create a `.env` file in the repository root:

```bash
# .env
OLLAMA_URL=http://your-server:11434
```

The agent will load this automatically.

---

## Usage

### Generate and Display Episodes

Generate a single episode and print to stdout:

```bash
python run.py --model gemma3:1b --episodes 1
```

### Generate and Save Episodes

Generate multiple episodes and save each to a separate Markdown file in the `episodes/` directory:

```bash
python run.py --model gemma3:1b --episodes 3 --save --out-dir episodes
```

### Custom Prompt

Provide your own prompt to guide episode generation:

```bash
python run.py --model gemma3:1b --prompt "Create a dramatic sci-fi script about colonists on Mars" --episodes 1 --save
```

### All CLI Options

```
usage: run.py [-h] [--model MODEL] [--prompt PROMPT] [--episodes EPISODES] 
              [--save] [--out-dir OUT_DIR]

options:
  -h, --help            show this help message and exit
  --model MODEL         Model name to use (default: gemma3:1b)
  --prompt PROMPT       Prompt to send to the model (default: Generate an 
                        original TV episode script inspired by The Expanse)
  --episodes EPISODES   Number of episodes to generate (default: 1)
  --save                Save generated episodes to files
  --out-dir OUT_DIR     Directory to save episodes (default: episodes)
```

### Example Commands

**Generate 5 episodes for The Expanse book series:**
```bash
python run.py --model gemma3:1b --episodes 5 --save
```

**Test connection with a simple prompt:**
```bash
python run.py --model gemma3:1b --prompt "Hello, what is your name?" --episodes 1
```

---

## Project Structure

```
agentic_python/
├── run.py                      # CLI entry point
├── requirements.txt            # Python dependencies
├── .gitignore                  # Git ignore rules
├── README.md                   # This file
├── episodes/                   # Generated Markdown episode files
└── src/
    └── agentic/
        ├── __init__.py         # Package marker
        ├── agent.py            # Episode generation and file writing logic
        └── ollama_client.py    # HTTP client for Ollama API
```

### Key Files

- **`src/agentic/ollama_client.py`**: Handles HTTP communication with Ollama's `/v1/completions` endpoint. Customize here for streaming, different endpoints, or retry logic.
- **`src/agentic/agent.py`**: Contains episode generation prompts and file writing. Modify `_build_episode_prompt()` to change script guidance.
- **`run.py`**: CLI interface. Add additional arguments or modes here.

---

## Troubleshooting

### Connection Error: "Connection refused"

**Problem:** `Failed to call Ollama: Connection refused`

**Solutions:**
1. Verify Ollama is running:
   ```bash
   curl http://127.0.0.1:11434/
   ```
2. Check the Ollama server address matches your setup (see [Configuration](#configuration)).
3. If Ollama runs on a different host/port, update `OLLAMA_URL` environment variable.

### Timeout Error: "Read timed out"

**Problem:** `HTTPConnectionPool(...): Read timed out`

**Solutions:**
1. The model is slow or the prompt is generating long responses. This is normal for smaller models like `gemma3:1b`.
2. Increase timeout in `src/agentic/ollama_client.py`:
   ```python
   resp = requests.post(url, json=payload, timeout=300)  # Increase to 300 seconds
   ```

### Model Not Found: "404 Not Found"

**Problem:** `404 Client Error: Not Found for url: http://127.0.0.1:11434/v1/models`

**Solutions:**
1. Verify you have a model installed in Ollama:
   ```bash
   ollama list
   ```
2. If no models appear, pull one:
   ```bash
   ollama pull gemma3:1b
   ```
3. Or use a different model you already have installed.

### No Output or Partial Output

**Problem:** Generated script is incomplete or missing.

**Solutions:**
1. Increase `max_tokens` in `src/agentic/agent.py` (current: 2000).
2. Use a larger, more capable model (e.g., `llama2` or `mistral`).
3. Check the `episodes/` directory—the file may have been saved.

---

## Notes

- **Smaller models** (like `gemma3:1b`) are faster but less detailed. For richer scripts, use larger models if your hardware supports them.
- **Response time** varies widely depending on your model and hardware. Allow 1–5 minutes per episode on typical machines.
- **File Writing** requires write permissions in the repository directory. Ensure the `episodes/` folder is writable.
- **Prompts** are customizable—modify `_build_episode_prompt()` in `src/agentic/agent.py` to adjust tone, length, or content guidance.

---

## License

This project is provided as-is. Feel free to extend and modify for your own use.
